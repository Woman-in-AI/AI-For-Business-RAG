import transformers
from sentence_transformers import SentenceTransformer
from faiss import IndexFlatL2
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings  # Updated import

def extract_text_from_pdf(file_path):
    """Extracts text from a PDF document using PyPDF2.

    Args:
        file_path: Path to the PDF file.

    Returns:
        str: Extracted text from the PDF.
    """
    import PyPDF2  # Import PyPDF2 within the function for better encapsulation
    try:
        with open(file_path, 'rb') as pdf_file:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text()
        return text
    except FileNotFoundError:
        print(f"Error: PDF file not found at '{file_path}'. Please check the file path.")
        return ""
    except PermissionError:
        print(f"Error: Permission denied while trying to read '{file_path}'.")
        return ""
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
        return ""

# Load the PDF document
text = extract_text_from_pdf("assignment.pdf")

# Create a list of documents
if text:
    documents = [Document(page_content=text)]
else:
    print("No text extracted from PDF.")

# Split documents into chunks (optional)
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(documents)

# Define LLM (using transformers library)
model_name = "facebook/bart-base"
llm = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

# Sentence transformer for embedding generation (using langchain_huggingface)
embedder = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")

# Create embeddings for documents
doc_embeddings = embedder.embed_documents([doc.page_content for doc in docs])  # Fixed list access

# Create FAISS index for efficient retrieval
index = IndexFlatL2(doc_embeddings.shape[1])
index.add(doc_embeddings)

def get_relevant_documents(query):
    """Retrieves the most relevant documents for a given query."""
    query_embedding = embedder.embed_query(query)
    distances, indices = index.search(query_embedding, k=1)
    relevant_docs = [docs[i] for i in indices[0]]
    return relevant_docs

# Ask a query
query = "What is the topic discussed?"
retrieved_docs = get_relevant_documents(query)

# Generate answer using LLM
prompt = f"Answer the question: '{query}'. Use the following context to generate a comprehensive and informative answer:\n\n" + "\n".join([doc.page_content for doc in retrieved_docs])

# Tokenize the prompt
encoded_prompt = tokenizer(prompt, return_tensors="pt")

# Generate text using LLM
generated_text = llm.generate(**encoded_prompt, max_length=50, min_length=20)

# Access the generated text from the output
print(generated_text[0]["generated_text"])
